---
title: Weighting Lifting Exercise prediction
  analysis
output:
  html_document:
    keep_md: yes
---

```{r message=FALSE}
library(caret)
library(gbm)
library(dplyr)
library(tidyverse)
```

## Data Loading
The data in this report comes from http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har. We start off by downloading the training and testing datasets into R.

```{r}
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile="pml-training.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile="pml-testing.csv")
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

## Data Cleanup
The original data is organized in a way that there are some measurement lines, followed by a summary line. The summary line contains a lot of NA / DIV!0 values with the measurements, and is denoted in the data by the new_window = 'yes' variable. Here we filter them out as summary is simply transformation of measurement statistics, and would be expected to be highly-correlated with them. Besides, we take out the independent variables used in our model.

```{r}
set.seed(12345)
training_clean <- training %>% filter(new_window == 'no') %>% select(starts_with("gyros"), starts_with("accel"), starts_with("magnet"), starts_with("classe")) 
```

## Create data subsets for cross-validation
We partition the training set into training and validation subsets for cross-validation purpose, assuming a 70-30 split.

```{r}
training_subIndex <- createDataPartition(y=training_clean$classe, p=0.7, list=FALSE)
training_sub <- training_clean[training_subIndex, ] 
validation_sub <- training_clean[-training_subIndex, ]
```

## Model Fitting and Validation
We will begin by fitting the training subset to 3 models, random forest, boosting and linear discriminant analysis. Then we validate the performance of our models by passing the validation subset to them. As can be seen below, random forest attained the highest level of accuracy from the validation subset, and better out-of-sample error than boosting and linear discriminant analysis models. 
As a reference, we also printed out the times so you may compare the relative time taken to build to models.

```{r}
Sys.time()
mod1 <- train(classe ~ ., method="rf", data=training_sub)
Sys.time()
mod2 <- train(classe ~ ., method="gbm", data=training_sub, verbose=FALSE)
Sys.time()
mod3 <- train(classe ~ ., method="lda", data=training_sub)
Sys.time()

pred1 <- predict(mod1, validation_sub)
pred2 <- predict(mod2, validation_sub)
pred3 <- predict(mod3, validation_sub)

confusionMatrix(as.factor(validation_sub$classe), pred1)
confusionMatrix(as.factor(validation_sub$classe), pred2)
confusionMatrix(as.factor(validation_sub$classe), pred3)
```

## Model Testing
Finally, we apply the selected model to the testing dataset. 

```{r}
predict(mod1, testing)
```
